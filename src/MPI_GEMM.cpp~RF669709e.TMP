#include <mpi.h>
#include <iostream>
#include "MPI_GEMM.h"
#include <chrono>
#include <iomanip>
#include <random>

//const int setwConst = 10;
//bool coutFlag = false;

//void simpleGEMM(int m, int n, int k, double* A, double* B, double* C, double alpha, double beta) { // colomn-major
//    int i, j, p;
//    for (i = 0; i < m; ++i) {
//        for (j = 0; j < n; ++j) {
//            C[j * m + i] *= beta;
//            for (p = 0; p < k; ++p) {
//                C[j * m + i] += alpha * A[p * m + i] * B[j * k + p];
//            }
//        }
//    }
//}

void simpleGEMM(int m, int n, int k, double* A, double* B, double* C) { // colomn-major
    int i, j, p;
    for (j = 0; j < n; ++j) {
        for (p = 0; p < k; ++p) {
            for (i = 0; i < m; ++i) {
                C[j * m + i] = std::fma(A[p * m + i], B[j * k + p], C[j * m + i]);
            }
        }
    }
}

//void determinedGenerate(int m, int n, double* M) {
//    for (size_t i = 0; i < m; ++i) {
//        for (size_t j = 0; j < n; ++j) {
//            M[j * m + i] = j * m + i;
//        }
//    }
//}

void simpleGenerate(int m, int n, double* M) {
    std::random_device r;
    std::default_random_engine e(r());
    std::uniform_real_distribution<double> coef_gen(-1.0, 1.0);
    double coef;

    for (size_t i = 0; i < m; ++i) {
        for (size_t j = 0; j < n; ++j) {
            M[j * m + i] = coef_gen(e);
        }
    }
}

//void printAs2D(int m, int n, double* M) {
//    std::cout << std::fixed << std::setprecision(2);
//    for (size_t i = 0; i < m; ++i) {
//        for (size_t j = 0; j < n; ++j) {
//            std::cout << std::setw(setwConst) << M[j * m + i];
//        }
//        std::cout << std::endl;
//    }
//}
//
//void printAs1D(int m, int n, double* M) {
//    std::cout << std::fixed << std::setprecision(2);
//    for (size_t i = 0; i < n * m; ++i) {
//        std::cout << std::setw(setwConst) << M[i];
//    }
//    std::cout << std::endl;
//}

double getErr(int m, int n, double* example, double* result) {
    double err = 0.0;
    for (size_t i = 0; i < m * n; ++i) {
        err = std::max(err, std::abs(example[i] - result[i]));
    }
    return err;
}

void gather_result_blocks(double* block, double* RES,
    int N, int q, int block_size, MPI_Comm grid_comm) {
    int rank;
    MPI_Comm_rank(grid_comm, &rank);

    double* gathered = nullptr;
    if (rank == 0) {
        gathered = new double[N * N];
    }

    MPI_Gather(block, block_size * block_size, MPI_DOUBLE,
        gathered, block_size * block_size, MPI_DOUBLE,
        0, grid_comm);

    if (rank == 0) {
        for (int p = 0; p < q * q; ++p) {
            int proc_row = p / q;
            int proc_col = p % q;

            for (int i = 0; i < block_size; ++i)
                for (int j = 0; j < block_size; ++j) {
                    int global_i = proc_row * block_size + i;
                    int global_j = proc_col * block_size + j;
                    RES[global_i * N + global_j] =
                        gathered[p * block_size * block_size + i * block_size + j];
                }
        }
    }

    delete[] gathered;
}

void distribute_matrix(double* full, double* local, int N, int q, int block_size) {
    double* blocks = new double[N * N];

    for (int proc = 0; proc < q * q; ++proc) {
        int proc_row = proc / q;
        int proc_col = proc % q;

        for (int i = 0; i < block_size; ++i)
            for (int j = 0; j < block_size; ++j) {
                int global_i = proc_row * block_size + i;
                int global_j = proc_col * block_size + j;
                blocks[proc * block_size * block_size + i * block_size + j] =
                    full[global_i * N + global_j];
            }
    }

    MPI_Scatter(blocks, block_size * block_size, MPI_DOUBLE,
        local, block_size * block_size, MPI_DOUBLE,
        0, MPI_COMM_WORLD);

    delete[] blocks;
}

void MPIGemm_old(int rank, int numtasks, MPI_Comm grid_comm, int dims[2], int periods[2], int coords[2], int block_size, double* M1, double* M2, double* M3, int q, double* RES) {

    //    if (rank == 1) {
    //        if (coutFlag) std::cout << "rank is: " << rank << std::endl;
    //        if (coutFlag) printAs2D(block_size, block_size, M1);
    //        if (coutFlag) std::cout << std::endl;
    //        if (coutFlag) printAs2D(block_size, block_size, M2);
    //        if (coutFlag) std::cout << std::endl;
    //    }

    int left, right, up, down;

    MPI_Cart_shift(grid_comm, 1, -coords[0], &right, &left);
    MPI_Sendrecv_replace(M2, block_size * block_size, MPI_DOUBLE,
        left, 0, right, 0, grid_comm, MPI_STATUS_IGNORE);

    MPI_Cart_shift(grid_comm, 0, -coords[1], &down, &up);
    MPI_Sendrecv_replace(M1, block_size * block_size, MPI_DOUBLE,
        up, 0, down, 0, grid_comm, MPI_STATUS_IGNORE);

    //    if (rank == 1) {
    //        if (coutFlag) std::cout << "rank is: " << rank << std::endl;
    //        if (coutFlag) printAs2D(block_size, block_size, M1);
    //        if (coutFlag) std::cout << std::endl;
    //        if (coutFlag) printAs2D(block_size, block_size, M2);
    //        if (coutFlag) std::cout << std::endl;
    //    }

    MPI_Cart_shift(grid_comm, 1, -1, &right, &left);
    MPI_Cart_shift(grid_comm, 0, -1, &down, &up);

    // Cannon's cycle
    for (int step = 0; step < q; ++step) {
        //        if (rank == 1) {
        //            if (coutFlag) std::cout << "rank is: " << rank << std::endl;
        //            if (coutFlag) printAs2D(block_size, block_size, M3);
        //            if (coutFlag) std::cout << std::endl;
        //        }

        simpleGEMM(block_size, block_size, block_size, M1, M2, M3);

        //        if (rank == 1) {
        //            if (coutFlag) std::cout << "rank is: " << rank << std::endl;
        //            if (coutFlag) printAs2D(block_size, block_size, M3);
        //            if (coutFlag) std::cout << std::endl;
        //        }

        MPI_Sendrecv_replace(M2, block_size * block_size, MPI_DOUBLE,
            left, 0, right, 0, grid_comm, MPI_STATUS_IGNORE);

        MPI_Sendrecv_replace(M1, block_size * block_size, MPI_DOUBLE,
            up, 0, down, 0, grid_comm, MPI_STATUS_IGNORE);
    }

    // Gather
    gather_result_blocks(M3, RES, q * block_size, q, block_size, grid_comm);
}

void MPI_GEMM_square(int argc, char** argv, int N, double* A, double* B, double* C) {
    // MPI initialization
    int numtasks, rank;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);

    // Getting sizes
    double* M1, * M2, * M3;
    int q = sqrt(numtasks);
    int blockSize = N / q;
    if (q * q != numtasks) {
        if (rank == 0)
            std::cerr << "Number of processes must be a perfect square\n";
        if (rank == 0 && q * blockSize != N) 
            std::cerr << "Number of processes must be matrice's divider\n";
        MPI_Finalize();
        return;
    }

    // Create 2D grid
    MPI_Comm grid_comm;
    int dims[2] = { q, q }, periods[2] = { 1, 1 }, coords[2];
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &grid_comm);
    MPI_Cart_coords(grid_comm, rank, 2, coords);

    // Memory allocation and distribution
    M1 = new double[blockSize * blockSize] {};
    M2 = new double[blockSize * blockSize] {};
    M3 = new double[blockSize * blockSize] {};
    if (rank == 0) {
        distribute_matrix(A, M1, N, q, blockSize);
        distribute_matrix(B, M2, N, q, blockSize);
    }
    else {
        MPI_Scatter(nullptr, blockSize * blockSize, MPI_DOUBLE,
            M1, blockSize * blockSize, MPI_DOUBLE,
            0, MPI_COMM_WORLD);
        MPI_Scatter(nullptr, blockSize * blockSize, MPI_DOUBLE,
            M2, blockSize * blockSize, MPI_DOUBLE,
            0, MPI_COMM_WORLD);
    }


    int left, right, up, down;
    MPI_Cart_shift(grid_comm, 1, -coords[0], &right, &left);
    MPI_Sendrecv_replace(M2, blockSize * blockSize, MPI_DOUBLE,
        left, 0, right, 0, grid_comm, MPI_STATUS_IGNORE);
    MPI_Cart_shift(grid_comm, 0, -coords[1], &down, &up);
    MPI_Sendrecv_replace(M1, blockSize * blockSize, MPI_DOUBLE,
        up, 0, down, 0, grid_comm, MPI_STATUS_IGNORE);

    //    if (rank == 1) {
    //        if (coutFlag) std::cout << "rank is: " << rank << std::endl;
    //        if (coutFlag) printAs2D(block_size, block_size, M1);
    //        if (coutFlag) std::cout << std::endl;
    //        if (coutFlag) printAs2D(block_size, block_size, M2);
    //        if (coutFlag) std::cout << std::endl;
    //    }

    MPI_Cart_shift(grid_comm, 1, -1, &right, &left);
    MPI_Cart_shift(grid_comm, 0, -1, &down, &up);

    // Cannon's cycle
    for (int step = 0; step < q; ++step) {
        //        if (rank == 1) {
        //            if (coutFlag) std::cout << "rank is: " << rank << std::endl;
        //            if (coutFlag) printAs2D(block_size, block_size, M3);
        //            if (coutFlag) std::cout << std::endl;
        //        }

        simpleGEMM(block_size, block_size, block_size, M1, M2, M3);

        //        if (rank == 1) {
        //            if (coutFlag) std::cout << "rank is: " << rank << std::endl;
        //            if (coutFlag) printAs2D(block_size, block_size, M3);
        //            if (coutFlag) std::cout << std::endl;
        //        }

        MPI_Sendrecv_replace(M2, block_size * block_size, MPI_DOUBLE,
            left, 0, right, 0, grid_comm, MPI_STATUS_IGNORE);

        MPI_Sendrecv_replace(M1, block_size * block_size, MPI_DOUBLE,
            up, 0, down, 0, grid_comm, MPI_STATUS_IGNORE);
    }

    // Gather
    gather_result_blocks(M3, RES, q * block_size, q, block_size, grid_comm);

    // Close MPI
    MPI_Finalize();
}

void testMPIGemm(int argc, char** argv) {
    int numtasks, rank;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);

    int q = sqrt(numtasks);
    int blockSize = 200;
    int N = q * blockSize;
    int n = 3;
    int m = 4;
    int k = 2;
    n = m = k = N;
    double alpha = 1.0;
    double beta = -1.0;
    std::chrono::steady_clock::time_point start, finish;
    uint64_t time;
    double* M1, * M2, * M3, * RES = nullptr, * A = nullptr, * B = nullptr, * C = nullptr;

    if (q * q != numtasks) {
        if (rank == 0)
            std::cerr << "Number of processes must be a perfect square\n";
        MPI_Finalize();
        return;
    }

    // Create 2D grid
    MPI_Comm grid_comm;
    int dims[2] = { q, q }, periods[2] = { 1, 1 }, coords[2];
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &grid_comm);
    MPI_Cart_coords(grid_comm, rank, 2, coords);

    M1 = new double[blockSize * blockSize] {};
    M2 = new double[blockSize * blockSize] {};
    M3 = new double[blockSize * blockSize] {};
    if (rank == 0) {
        RES = new double[N * N] {};
        A = new double[N * N];
        B = new double[N * N];
        simpleGenerate(N, N, A);
        simpleGenerate(N, N, B);
        distribute_matrix(A, M1, N, q, blockSize);
        distribute_matrix(B, M2, N, q, blockSize);
    }
    else {
        MPI_Scatter(nullptr, blockSize * blockSize, MPI_DOUBLE,
            M1, blockSize * blockSize, MPI_DOUBLE,
            0, MPI_COMM_WORLD);
        MPI_Scatter(nullptr, blockSize * blockSize, MPI_DOUBLE,
            M2, blockSize * blockSize, MPI_DOUBLE,
            0, MPI_COMM_WORLD);
    }

//    if (coutFlag) std::cout << "rank: " << rank << std::endl;
//    if (coutFlag) printAs2D(blockSize, blockSize, M1);
//    if (coutFlag) std::cout << std::endl;
//    if (coutFlag) printAs2D(blockSize, blockSize, M2);
//    if (coutFlag) std::cout << std::endl;

    // Distribute matrices to 2D mesh ...
    start = std::chrono::steady_clock::now();
    MPIGemm(rank, numtasks, grid_comm, dims, periods, coords, blockSize, M1, M2, M3, q, RES);
    finish = std::chrono::steady_clock::now();
    time = std::chrono::duration_cast<std::chrono::milliseconds> (finish - start).count();
    // double MPI_Wtime() ...

    std::cout << "Time is: " << time << std::endl;

    delete[] M1;
    delete[] M2;
    delete[] M3;

    if (rank == 0) {
//        if (coutFlag) printAs2D(N, N, A);
//        if (coutFlag) std::cout << std::endl;
//        if (coutFlag) printAs2D(N, N, B);
//        if (coutFlag) std::cout << std::endl;
        C = new double[N * N] {};

        start = std::chrono::steady_clock::now();
        simpleGEMM(N, N, N, A, B, C);
        finish = std::chrono::steady_clock::now();
        time = std::chrono::duration_cast<std::chrono::milliseconds> (finish - start).count();
        std::cout << std::endl << "Time for naive implementation: " << time << " , error is: " << getErr(N, N, C, RES) << std::endl;
//        if (coutFlag) printAs2D(N, N, C);
//        if (coutFlag) std::cout << std::endl;
//        if (coutFlag) printAs2D(N, N, RES);
//        if (coutFlag) std::cout << std::endl;
    }

    delete[] RES;
    delete[] A;
    delete[] B;
    delete[] C;

    MPI_Finalize();
}